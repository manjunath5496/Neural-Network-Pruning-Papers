<h2> Neural Network Pruning Papers </h2>






<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(1).pdf" style="text-decoration:none;">Learning bothWeights and Connections for Efficient Neural Networks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(2).pdf" style="text-decoration:none;">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(3).pdf" style="text-decoration:none;">Dynamic Network Surgery for Efficient DNNs</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(4).pdf" style="text-decoration:none;">Pruning Filters for Efficient ConvNets</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(5).pdf" style="text-decoration:none;">Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(6).pdf" style="text-decoration:none;">Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(7).pdf" style="text-decoration:none;">Pruning Convolutional Neural Networks for Resource Efficient Inference</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(8).pdf" style="text-decoration:none;"> Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(9).pdf" style="text-decoration:none;">Channel Pruning for Accelerating Very Deep Neural Networks</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(10).pdf" style="text-decoration:none;">ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(11).pdf" style="text-decoration:none;">Learning Efficient Convolutional Networks through Network Slimming</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(12).pdf" style="text-decoration:none;">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks </a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(13).pdf" style="text-decoration:none;">Dynamic Sparse Graph for Efficient Deep Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(14).pdf" style="text-decoration:none;">SNIP: Single-shot Network Pruning based on Connection Sensitivity</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(15).pdf" style="text-decoration:none;">Rethinking the Value of Network Pruning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(16).pdf" style="text-decoration:none;">Dynamic Channel Pruning: Feature Boosting and Suppression</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(17).pdf" style="text-decoration:none;">Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(18).pdf" style="text-decoration:none;">Structured Pruning of Neural Networks with Budget-Aware Regularization</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(19).pdf" style="text-decoration:none;">On Implicit Filter Level Sparsity in Convolutional Neural Networks</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(20).pdf" style="text-decoration:none;">Accelerate CNN via Recursive Bayesian Pruning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(21).pdf" style="text-decoration:none;">Few Sample Knowledge Distillation for Efficient Network Compression</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(22).pdf" style="text-decoration:none;">Model Compression with Adversarial Robustness: A Unified Optimization Framework</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(23).pdf" style="text-decoration:none;">Partial Order Pruning: for Best Speed/Accuracy Trade-off in Neural Architecture Search</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(24).pdf" style="text-decoration:none;">Towards Optimal Structured CNN Pruning via Generative Adversarial Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(25).pdf" style="text-decoration:none;">MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(26).pdf" style="text-decoration:none;">Adversarial Robustness vs. Model Compression, or Both?</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(27).pdf" style="text-decoration:none;">Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(28).pdf" style="text-decoration:none;">Towards Efficient Model Compression via Learned Global Ranking</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(29).pdf" style="text-decoration:none;">Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(30).pdf" style="text-decoration:none;">Approximated Oracle Filter Pruning for Destructive CNN Width Optimization</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(31).pdf" style="text-decoration:none;">EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(32).pdf" style="text-decoration:none;">Network Pruning via
Transformable Architecture Search</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(33).pdf" style="text-decoration:none;">Structured Compression by Weight Encryption for Unstructured Pruning and Quantization</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(34).pdf" style="text-decoration:none;">OICSR: Out-In-Channel Sparsity Regularization for Compact Deep Neural Networks</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(35).pdf" style="text-decoration:none;">One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(36).pdf" style="text-decoration:none;">A Signal Propagation Perspective for Pruning Neural Networks at Initialization</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(37).pdf" style="text-decoration:none;">AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(38).pdf" style="text-decoration:none;">Data-Independent Neural Pruning via Coresets</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(39).pdf" style="text-decoration:none;">Accelerating CNN Training by Pruning Activation Gradients</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(40).pdf" style="text-decoration:none;">Adversarial Neural Pruning with Latent Vulnerability Suppression</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(41).pdf" style="text-decoration:none;">Learning Filter Basis for Convolutional Neural Network Compression</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(42).pdf" style="text-decoration:none;">Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(43).pdf" style="text-decoration:none;">Pruning from Scratch</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(44).pdf" style="text-decoration:none;">Global Sparse Momentum SGD for Pruning Very Deep Neural Networks</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(45).pdf" style="text-decoration:none;">Provable Filter Pruning for Efficient Neural Networks </a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(46).pdf" style="text-decoration:none;">DARB: A Density-Adaptive Regular-Block Pruning for Deep Neural Networks</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(47).pdf" style="text-decoration:none;">Neural Network Pruning with Residual-Connections and Limited-Data</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(48).pdf" style="text-decoration:none;">One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(49).pdf" style="text-decoration:none;">Channel Pruning via Automatic Structure Search</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(50).pdf" style="text-decoration:none;">Proving the Lottery Ticket Hypothesis: Pruning is All You Need</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(51).pdf" style="text-decoration:none;">Soft ThresholdWeight Reparameterization for Learnable Sparsity</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(52).pdf" style="text-decoration:none;">Lookahead: A Far-Sighted Alternative of Magnitude-based Pruning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(53).pdf" style="text-decoration:none;">HRank: Filter Pruning using High-Rank Feature Map</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(54).pdf" style="text-decoration:none;">Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(55).pdf" style="text-decoration:none;">Comparing Rewinding and Fine-tuning in Neural Network Pruning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(56).pdf" style="text-decoration:none;">Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(57).pdf" style="text-decoration:none;">DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(58).pdf" style="text-decoration:none;">DHP: Differentiable Meta Pruning via HyperNetworks</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(59).pdf" style="text-decoration:none;">DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(60).pdf" style="text-decoration:none;">DMCP: Differentiable Markov Channel Pruning for Neural Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(61).pdf" style="text-decoration:none;">EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(62).pdf" style="text-decoration:none;">Meta-Learning with Network Pruning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(63).pdf" style="text-decoration:none;">Operation-Aware Soft Channel Pruning using Differentiable Masks</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(64).pdf" style="text-decoration:none;">Differentiable Joint Pruning and Quantization for Hardware Efficiency</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(65).pdf" style="text-decoration:none;">DropNet: Reducing Neural Network Complexity via Iterative Pruning</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(66).pdf" style="text-decoration:none;">Runtime Neural Pruning</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(67).pdf" style="text-decoration:none;">Dynamic Model Pruning with Feedback</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(68).pdf" style="text-decoration:none;">Discrete Model Compression with Resource Constraint for Deep Neural Networks</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(69).pdf" style="text-decoration:none;">Multi-Dimensional Pruning: A Unified Framework for Model Compression</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(70).pdf" style="text-decoration:none;">Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(71).pdf" style="text-decoration:none;">Importance Estimation for Neural Network Pruning</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(72).pdf" style="text-decoration:none;">AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(73).pdf" style="text-decoration:none;">Collaborative Channel Pruning for Deep Networks</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(74).pdf" style="text-decoration:none;">ProxSGD: Training Structured Neural Networks under Regularization and Constraints</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Network-Pruning-Papers/blob/master/nnp(75).pdf" style="text-decoration:none;">Variational Convolutional Neural Network Pruning</a></li>                        
</ul>
  
  
  
